{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "from torch.utils.data import TensorDataset, DataLoader, RandomSampler\n",
    "from torch.utils.tensorboard import SummaryWriter\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "device = torch.device('cuda') if torch.cuda.is_available() else torch.device('cpu')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def pcg3dFloat(v):\n",
    "\t# Range of uint values: [0, 4294967295]\n",
    "    v = v * np.uint32(1664525) + np.uint32(1013904223)\n",
    "\n",
    "    v[..., 0] += v[..., 1] * v[..., 2]\n",
    "    v[..., 1] += v[..., 2] * v[..., 0]\n",
    "    v[..., 2] += v[..., 0] * v[..., 1]\n",
    "\n",
    "    v ^= v >> np.uint32(16)\n",
    "    \n",
    "    v[..., 0] += v[..., 1] * v[..., 2]\n",
    "    v[..., 1] += v[..., 2] * v[..., 0]\n",
    "    v[..., 2] += v[..., 0] * v[..., 1]\n",
    "\n",
    "    return (v * (1.0 / 4294967296.0)).astype(np.float32)\n",
    "\n",
    "def normalise_linear(arr):\n",
    "    return (arr - np.min(arr)) / (np.max(arr) - np.min(arr))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class MLP(nn.Module):\n",
    "    def __init__(self, input_size, output_size):\n",
    "        super(MLP, self).__init__()        \n",
    "        self.model = nn.Sequential(\n",
    "            nn.Linear(input_size, 256),\n",
    "            nn.LeakyReLU(0.01),\n",
    "            nn.Linear(256, 512),\n",
    "            nn.LeakyReLU(0.01),\n",
    "            nn.Linear(512, 1024),\n",
    "            nn.LeakyReLU(0.01),\n",
    "            nn.Linear(1024, 512),\n",
    "            nn.LeakyReLU(0.01),\n",
    "            nn.Linear(512, 256),\n",
    "            nn.LeakyReLU(0.01),\n",
    "            nn.Linear(256, 128),\n",
    "            nn.LeakyReLU(0.01),\n",
    "            nn.Linear(128, output_size),\n",
    "            nn.Sigmoid()\n",
    "        )\n",
    "        \n",
    "    def forward(self, x):\n",
    "        x = self.model(x)\n",
    "        \n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Generata raw data\n",
    "n = int(1000000)\n",
    "inputs = np.random.randint(0, 4294967296, size=(n,3), dtype=np.uint32)\n",
    "inputs_normalised = torch.from_numpy(normalise_linear(inputs).astype(np.float32)).to(device)\n",
    "outputs = torch.from_numpy(pcg3dFloat(inputs)).to(device)\n",
    "\n",
    "# Batch data for training\n",
    "dataset = TensorDataset(inputs_normalised, outputs)\n",
    "sampler = RandomSampler(dataset)\n",
    "# My CPU has 10 cores. Checked by sysctl -n hw.ncpu.\n",
    "dataLoader = DataLoader(dataset, batch_size=32768, sampler=sampler, num_workers=5)\n",
    "# Define model, loss function and optimizer\n",
    "model = MLP(3, 3).to(device)\n",
    "mse = nn.MSELoss()\n",
    "mae = nn.L1Loss()\n",
    "LOSS = ['MAE', 'MSE'][0]\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=0.001)\n",
    "scheduler = torch.optim.lr_scheduler.StepLR(optimizer, step_size=1000, gamma=0.1)\n",
    "# scheduler = torch.optim.lr_scheduler.ReduceLROnPlateau(optimizer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialize SummaryWriter\n",
    "writer = SummaryWriter()\n",
    "loss_fn = mae if LOSS == 'MAE' else mse"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Training loop\n",
    "epochs = 4000\n",
    "global_step = 0.0  # Global step counter for accurate batch-level logging\n",
    "\n",
    "for epoch in range(epochs):\n",
    "    epoch_loss = 0.0\n",
    "    num_batches = len(dataLoader)\n",
    "    \n",
    "    for batch_inputs, batch_outputs in dataLoader:\n",
    "        outputs = model(batch_inputs)\n",
    "        loss = loss_fn(outputs, batch_outputs)\n",
    "        \n",
    "        # Accumulate loss for averaging later\n",
    "        epoch_loss += loss.item()\n",
    "        \n",
    "        # TensorBoard logging for each batch\n",
    "        # writer.add_scalar(f'{LOSS} Loss/train', loss.item(), global_step)\n",
    "        global_step += 1\n",
    "        \n",
    "        # Backpropagation\n",
    "        optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "    \n",
    "    # Logging the average loss per epoch\n",
    "    avg_epoch_loss = epoch_loss / num_batches\n",
    "    writer.add_scalar(f'{LOSS} Loss/epoch', avg_epoch_loss, epoch)\n",
    "    \n",
    "    # Learning rate scheduling\n",
    "    scheduler.step()\n",
    "    # scheduler.step(avg_epoch_loss)\n",
    "    \n",
    "    print(f'Epoch {epoch+1}/{epochs}, Avg Epoch Loss: {avg_epoch_loss:,}')\n",
    "\n",
    "# Close the writer after training\n",
    "writer.flush()\n",
    "writer.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pcg3dFloat(inputs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model(inputs_normalised).detach().cpu().numpy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "dmodel",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
